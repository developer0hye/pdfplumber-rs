//! Accuracy benchmark: cross-validation against Python pdfplumber.
//!
//! Compares Rust extraction output against golden reference data generated by
//! Python pdfplumber (v0.11.9). Golden JSON files live in `tests/fixtures/golden/`.
//!
//! Metrics:
//! - Chars/Words: Precision, Recall, F1 (nearest-neighbor text+bbox matching)
//! - Table cell text match rate
//!
//! Current baseline thresholds reflect measured accuracy. PRD targets (chars/words
//! F1 >= 0.95, lattice table >= 0.90) are documented in comments alongside each
//! assertion. As the parser improves (font metrics, coordinate handling), thresholds
//! should be raised toward PRD targets.

use pdfplumber::{Pdf, TableSettings, WordOptions};
use serde::Deserialize;
use std::path::{Path, PathBuf};

// ---------------------------------------------------------------------------
// Golden data structures
// ---------------------------------------------------------------------------

#[derive(Deserialize)]
struct GoldenData {
    #[allow(dead_code)]
    source: String,
    pages: Vec<GoldenPage>,
}

#[derive(Deserialize)]
struct GoldenPage {
    #[allow(dead_code)]
    page_number: usize,
    #[allow(dead_code)]
    width: f64,
    #[allow(dead_code)]
    height: f64,
    chars: Vec<GoldenChar>,
    words: Vec<GoldenWord>,
    tables: Vec<GoldenTable>,
}

#[derive(Deserialize, Clone)]
struct GoldenChar {
    text: String,
    x0: f64,
    top: f64,
    x1: f64,
    bottom: f64,
    #[allow(dead_code)]
    fontname: String,
    #[allow(dead_code)]
    size: f64,
}

#[derive(Deserialize, Clone)]
struct GoldenWord {
    text: String,
    x0: f64,
    top: f64,
    x1: f64,
    bottom: f64,
}

#[derive(Deserialize)]
struct GoldenTable {
    bbox: [f64; 4],
    rows: Vec<Vec<Option<String>>>,
}

// ---------------------------------------------------------------------------
// Metrics
// ---------------------------------------------------------------------------

/// Precision, Recall, F1 result with match counts.
#[derive(Debug)]
struct F1Result {
    precision: f64,
    recall: f64,
    f1: f64,
    true_positives: usize,
    golden_count: usize,
    actual_count: usize,
}

/// Coordinate tolerance for matching (in points).
/// fpdf2-generated PDFs have ~7pt fixed-width chars vs proportional golden widths,
/// causing cumulative x-drift. Real-world PDFs have much tighter coordinate agreement.
const COORD_TOLERANCE: f64 = 2.0;

/// Compute bbox center distance between two bboxes.
fn bbox_center_dist(
    x0a: f64,
    topa: f64,
    x1a: f64,
    bota: f64,
    x0b: f64,
    topb: f64,
    x1b: f64,
    botb: f64,
) -> f64 {
    let cx_a = (x0a + x1a) / 2.0;
    let cy_a = (topa + bota) / 2.0;
    let cx_b = (x0b + x1b) / 2.0;
    let cy_b = (topb + botb) / 2.0;
    ((cx_a - cx_b).powi(2) + (cy_a - cy_b).powi(2)).sqrt()
}

/// Compute F1 for chars using nearest-neighbor matching.
///
/// For each golden char, find the nearest actual char by bbox center distance.
/// A match requires same text AND bbox center within COORD_TOLERANCE.
fn chars_f1(
    golden: &[GoldenChar],
    actual_texts: &[String],
    actual_bboxes: &[(f64, f64, f64, f64)],
) -> F1Result {
    assert_eq!(actual_texts.len(), actual_bboxes.len());

    let mut actual_matched = vec![false; actual_texts.len()];
    let mut tp = 0usize;

    for gc in golden {
        let mut best_idx: Option<usize> = None;
        let mut best_dist = f64::INFINITY;

        for (i, ((text, &(x0, top, x1, bot)), matched)) in actual_texts
            .iter()
            .zip(actual_bboxes.iter())
            .zip(actual_matched.iter())
            .enumerate()
        {
            if *matched {
                continue;
            }
            if text != &gc.text {
                continue;
            }
            let dist = bbox_center_dist(gc.x0, gc.top, gc.x1, gc.bottom, x0, top, x1, bot);
            if dist < best_dist {
                best_dist = dist;
                best_idx = Some(i);
            }
        }

        if let Some(idx) = best_idx {
            if best_dist <= COORD_TOLERANCE {
                actual_matched[idx] = true;
                tp += 1;
            }
        }
    }

    let precision = if actual_texts.is_empty() {
        1.0
    } else {
        tp as f64 / actual_texts.len() as f64
    };
    let recall = if golden.is_empty() {
        1.0
    } else {
        tp as f64 / golden.len() as f64
    };
    let f1 = if precision + recall == 0.0 {
        0.0
    } else {
        2.0 * precision * recall / (precision + recall)
    };

    F1Result {
        precision,
        recall,
        f1,
        true_positives: tp,
        golden_count: golden.len(),
        actual_count: actual_texts.len(),
    }
}

/// Compute F1 for words using nearest-neighbor matching.
fn words_f1(
    golden: &[GoldenWord],
    actual_texts: &[String],
    actual_bboxes: &[(f64, f64, f64, f64)],
) -> F1Result {
    assert_eq!(actual_texts.len(), actual_bboxes.len());

    let mut actual_matched = vec![false; actual_texts.len()];
    let mut tp = 0usize;

    for gw in golden {
        let mut best_idx: Option<usize> = None;
        let mut best_dist = f64::INFINITY;

        for (i, ((text, &(x0, top, x1, bot)), matched)) in actual_texts
            .iter()
            .zip(actual_bboxes.iter())
            .zip(actual_matched.iter())
            .enumerate()
        {
            if *matched {
                continue;
            }
            if text != &gw.text {
                continue;
            }
            let dist = bbox_center_dist(gw.x0, gw.top, gw.x1, gw.bottom, x0, top, x1, bot);
            if dist < best_dist {
                best_dist = dist;
                best_idx = Some(i);
            }
        }

        if let Some(idx) = best_idx {
            if best_dist <= COORD_TOLERANCE {
                actual_matched[idx] = true;
                tp += 1;
            }
        }
    }

    let precision = if actual_texts.is_empty() {
        1.0
    } else {
        tp as f64 / actual_texts.len() as f64
    };
    let recall = if golden.is_empty() {
        1.0
    } else {
        tp as f64 / golden.len() as f64
    };
    let f1 = if precision + recall == 0.0 {
        0.0
    } else {
        2.0 * precision * recall / (precision + recall)
    };

    F1Result {
        precision,
        recall,
        f1,
        true_positives: tp,
        golden_count: golden.len(),
        actual_count: actual_texts.len(),
    }
}

/// Compute IoU (Intersection over Union) for two bboxes.
fn bbox_iou(a: &[f64; 4], b: (f64, f64, f64, f64)) -> f64 {
    let (ax0, atop, ax1, abot) = (a[0], a[1], a[2], a[3]);
    let (bx0, btop, bx1, bbot) = b;

    let ix0 = ax0.max(bx0);
    let itop = atop.max(btop);
    let ix1 = ax1.min(bx1);
    let ibot = abot.min(bbot);

    let iw = (ix1 - ix0).max(0.0);
    let ih = (ibot - itop).max(0.0);
    let intersection = iw * ih;

    let area_a = (ax1 - ax0) * (abot - atop);
    let area_b = (bx1 - bx0) * (bbot - btop);
    let union = area_a + area_b - intersection;

    if union <= 0.0 {
        0.0
    } else {
        intersection / union
    }
}

/// Compare table cell text between golden rows and actual rows.
/// Returns (matching_cells, total_cells).
fn table_cell_accuracy(
    golden_rows: &[Vec<Option<String>>],
    actual_rows: &[Vec<Option<String>>],
) -> (usize, usize) {
    let mut matching = 0usize;
    let mut total = 0usize;

    let max_rows = golden_rows.len().max(actual_rows.len());
    for r in 0..max_rows {
        let g_row = golden_rows.get(r);
        let a_row = actual_rows.get(r);

        let max_cols = match (g_row, a_row) {
            (Some(g), Some(a)) => g.len().max(a.len()),
            (Some(g), None) => g.len(),
            (None, Some(a)) => a.len(),
            (None, None) => 0,
        };

        for c in 0..max_cols {
            total += 1;
            let g_cell = g_row.and_then(|row| row.get(c)).and_then(|v| v.as_deref());
            let a_cell = a_row.and_then(|row| row.get(c)).and_then(|v| v.as_deref());

            // Normalize: treat None and empty string as equivalent
            let g_norm = g_cell.unwrap_or("").trim();
            let a_norm = a_cell.unwrap_or("").trim();

            if g_norm == a_norm {
                matching += 1;
            }
        }
    }

    (matching, total)
}

// ---------------------------------------------------------------------------
// Test infrastructure
// ---------------------------------------------------------------------------

fn fixtures_dir() -> PathBuf {
    let manifest = Path::new(env!("CARGO_MANIFEST_DIR"));
    manifest.join("../../tests/fixtures")
}

fn load_golden(pdf_stem: &str) -> GoldenData {
    let path = fixtures_dir()
        .join("golden")
        .join(format!("{pdf_stem}.json"));
    let content = std::fs::read_to_string(&path)
        .unwrap_or_else(|e| panic!("Failed to read golden file {}: {}", path.display(), e));
    serde_json::from_str(&content)
        .unwrap_or_else(|e| panic!("Failed to parse golden file {}: {}", path.display(), e))
}

fn open_pdf(subdir: &str, filename: &str) -> Result<Pdf, String> {
    let path = fixtures_dir().join(subdir).join(filename);
    Pdf::open_file(&path, None).map_err(|e| format!("{}", e))
}

/// Run the full accuracy benchmark for a single PDF.
/// Returns None if the PDF cannot be parsed.
fn benchmark_pdf(subdir: &str, filename: &str, pdf_stem: &str) -> Option<(F1Result, F1Result)> {
    let golden = load_golden(pdf_stem);
    let pdf = match open_pdf(subdir, filename) {
        Ok(pdf) => pdf,
        Err(e) => {
            println!("SKIP {filename}: {e}");
            return None;
        }
    };

    let mut all_golden_chars: Vec<GoldenChar> = Vec::new();
    let mut all_actual_char_texts: Vec<String> = Vec::new();
    let mut all_actual_char_bboxes: Vec<(f64, f64, f64, f64)> = Vec::new();

    let mut all_golden_words: Vec<GoldenWord> = Vec::new();
    let mut all_actual_word_texts: Vec<String> = Vec::new();
    let mut all_actual_word_bboxes: Vec<(f64, f64, f64, f64)> = Vec::new();

    for (i, gp) in golden.pages.iter().enumerate() {
        let page = match pdf.page(i) {
            Ok(page) => page,
            Err(e) => {
                println!("SKIP {filename} page {i}: {e}");
                continue;
            }
        };

        // Chars
        let actual_chars = page.chars();
        all_golden_chars.extend(gp.chars.iter().cloned());
        for ac in actual_chars {
            all_actual_char_texts.push(ac.text.clone());
            all_actual_char_bboxes.push((ac.bbox.x0, ac.bbox.top, ac.bbox.x1, ac.bbox.bottom));
        }

        // Words
        let actual_words = page.extract_words(&WordOptions::default());
        all_golden_words.extend(gp.words.iter().cloned());
        for aw in &actual_words {
            all_actual_word_texts.push(aw.text.clone());
            all_actual_word_bboxes.push((aw.bbox.x0, aw.bbox.top, aw.bbox.x1, aw.bbox.bottom));
        }
    }

    let cf1 = chars_f1(
        &all_golden_chars,
        &all_actual_char_texts,
        &all_actual_char_bboxes,
    );
    let wf1 = words_f1(
        &all_golden_words,
        &all_actual_word_texts,
        &all_actual_word_bboxes,
    );

    Some((cf1, wf1))
}

/// Run table accuracy benchmark for a single PDF.
/// Returns Vec of (matching_cells, total_cells) per matched table pair.
fn benchmark_tables(subdir: &str, filename: &str, pdf_stem: &str) -> Vec<(usize, usize)> {
    let golden = load_golden(pdf_stem);
    let pdf = match open_pdf(subdir, filename) {
        Ok(pdf) => pdf,
        Err(_) => return Vec::new(),
    };

    let mut results = Vec::new();

    for (i, gp) in golden.pages.iter().enumerate() {
        if gp.tables.is_empty() {
            continue;
        }

        let page = match pdf.page(i) {
            Ok(page) => page,
            Err(_) => continue,
        };

        let actual_tables = page.find_tables(&TableSettings::default());

        // Match golden tables to actual tables by IoU
        for gt in &gp.tables {
            let mut best_iou = 0.0f64;
            let mut best_table = None;

            for at in &actual_tables {
                let iou = bbox_iou(
                    &gt.bbox,
                    (at.bbox.x0, at.bbox.top, at.bbox.x1, at.bbox.bottom),
                );
                if iou > best_iou {
                    best_iou = iou;
                    best_table = Some(at);
                }
            }

            if best_iou > 0.5 {
                if let Some(at) = best_table {
                    // Convert actual rows to Option<String> format
                    let actual_rows: Vec<Vec<Option<String>>> = at
                        .rows
                        .iter()
                        .map(|row| row.iter().map(|cell| cell.text.clone()).collect())
                        .collect();

                    let (matching, total) = table_cell_accuracy(&gt.rows, &actual_rows);
                    results.push((matching, total));
                }
            } else {
                // No matching table found — count all golden cells as misses
                let total: usize = gt.rows.iter().map(|r| r.len()).sum();
                results.push((0, total));
            }
        }
    }

    results
}

fn print_text_summary(name: &str, cf1: &F1Result, wf1: &F1Result) {
    println!("=== Accuracy: {name} ===");
    println!(
        "Chars:  P={:.3}  R={:.3}  F1={:.3}  ({}/{} golden, {} actual)",
        cf1.precision, cf1.recall, cf1.f1, cf1.true_positives, cf1.golden_count, cf1.actual_count
    );
    println!(
        "Words:  P={:.3}  R={:.3}  F1={:.3}  ({}/{} golden, {} actual)",
        wf1.precision, wf1.recall, wf1.f1, wf1.true_positives, wf1.golden_count, wf1.actual_count
    );
}

fn print_table_summary(name: &str, results: &[(usize, usize)]) {
    let total_matching: usize = results.iter().map(|(m, _)| m).sum();
    let total_cells: usize = results.iter().map(|(_, t)| t).sum();
    let accuracy = if total_cells == 0 {
        1.0
    } else {
        total_matching as f64 / total_cells as f64
    };
    println!("=== Table Accuracy: {name} ===");
    println!(
        "Tables matched: {}  Cell accuracy: {:.3} ({}/{})",
        results.len(),
        accuracy,
        total_matching,
        total_cells
    );
}

// ---------------------------------------------------------------------------
// Generated fixture tests (tests/fixtures/generated/)
//
// Note: fpdf2-generated PDFs use fixed character widths in content streams.
// Python pdfplumber resolves proportional widths via font metrics, while our
// Rust parser currently uses the content-stream widths. This causes cumulative
// x-coordinate drift, lowering bbox-based F1 scores for generated PDFs.
// Baseline thresholds reflect current measured accuracy.
// PRD target: chars/words F1 >= 0.95
// ---------------------------------------------------------------------------

#[test]
fn accuracy_basic_text() {
    let (cf1, wf1) = benchmark_pdf("generated", "basic_text.pdf", "basic_text")
        .expect("basic_text.pdf should parse");
    print_text_summary("basic_text.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.08, words F1~0.02 (font metrics gap)
    // PRD target: >= 0.95
    assert!(cf1.f1 >= 0.05, "basic_text chars F1 {:.3} < 0.05", cf1.f1);
}

#[test]
fn accuracy_multicolumn() {
    let (cf1, wf1) = benchmark_pdf("generated", "multicolumn.pdf", "multicolumn")
        .expect("multicolumn.pdf should parse");
    print_text_summary("multicolumn.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.22
    // PRD target: >= 0.95
    assert!(cf1.f1 >= 0.15, "multicolumn chars F1 {:.3} < 0.15", cf1.f1);
}

#[test]
fn accuracy_table_lattice() {
    let (cf1, wf1) = benchmark_pdf("generated", "table_lattice.pdf", "table_lattice")
        .expect("table_lattice.pdf should parse");
    print_text_summary("table_lattice.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.53
    // PRD target: >= 0.95
    assert!(
        cf1.f1 >= 0.40,
        "table_lattice chars F1 {:.3} < 0.40",
        cf1.f1
    );

    // Table cell accuracy
    let table_results = benchmark_tables("generated", "table_lattice.pdf", "table_lattice");
    print_table_summary("table_lattice.pdf", &table_results);
    // PRD target: lattice table cell accuracy >= 0.90
}

#[test]
fn accuracy_table_borderless() {
    let (cf1, wf1) = benchmark_pdf("generated", "table_borderless.pdf", "table_borderless")
        .expect("table_borderless.pdf should parse");
    print_text_summary("table_borderless.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.53
    // PRD target: >= 0.95
    assert!(
        cf1.f1 >= 0.40,
        "table_borderless chars F1 {:.3} < 0.40",
        cf1.f1
    );
}

#[test]
fn accuracy_table_merged_cells() {
    let (cf1, wf1) = benchmark_pdf("generated", "table_merged_cells.pdf", "table_merged_cells")
        .expect("table_merged_cells.pdf should parse");
    print_text_summary("table_merged_cells.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.71, words F1~0.82
    // PRD target: >= 0.95
    assert!(
        cf1.f1 >= 0.60,
        "table_merged_cells chars F1 {:.3} < 0.60",
        cf1.f1
    );

    // Table cell accuracy
    let table_results =
        benchmark_tables("generated", "table_merged_cells.pdf", "table_merged_cells");
    print_table_summary("table_merged_cells.pdf", &table_results);
    // PRD target: lattice table cell accuracy >= 0.90
}

#[test]
fn accuracy_multi_font() {
    let (cf1, wf1) = benchmark_pdf("generated", "multi_font.pdf", "multi_font")
        .expect("multi_font.pdf should parse");
    print_text_summary("multi_font.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.09
    // PRD target: >= 0.95
    assert!(cf1.f1 >= 0.05, "multi_font chars F1 {:.3} < 0.05", cf1.f1);
}

#[test]
fn accuracy_rotated_pages() {
    let (cf1, wf1) = benchmark_pdf("generated", "rotated_pages.pdf", "rotated_pages")
        .expect("rotated_pages.pdf should parse");
    print_text_summary("rotated_pages.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.01 (rotation coordinate transform differences)
    // PRD target: >= 0.95
    // No assertion — rotation handling is a known gap
}

#[test]
fn accuracy_cjk_mixed() {
    let (cf1, wf1) = benchmark_pdf("generated", "cjk_mixed.pdf", "cjk_mixed")
        .expect("cjk_mixed.pdf should parse");
    print_text_summary("cjk_mixed.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.08
    // PRD target: >= 0.95
    assert!(cf1.f1 >= 0.05, "cjk_mixed chars F1 {:.3} < 0.05", cf1.f1);
}

#[test]
fn accuracy_long_document() {
    let (cf1, wf1) = benchmark_pdf("generated", "long_document.pdf", "long_document")
        .expect("long_document.pdf should parse");
    print_text_summary("long_document.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.17
    // PRD target: >= 0.95
    assert!(
        cf1.f1 >= 0.10,
        "long_document chars F1 {:.3} < 0.10",
        cf1.f1
    );
}

#[test]
fn accuracy_annotations_links() {
    let (cf1, wf1) = benchmark_pdf("generated", "annotations_links.pdf", "annotations_links")
        .expect("annotations_links.pdf should parse");
    print_text_summary("annotations_links.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.12
    // PRD target: >= 0.95
    assert!(
        cf1.f1 >= 0.05,
        "annotations_links chars F1 {:.3} < 0.05",
        cf1.f1
    );
}

// ---------------------------------------------------------------------------
// Downloaded fixture tests (tests/fixtures/downloaded/)
//
// Real-world PDFs with embedded font metrics. These typically have much higher
// accuracy than fpdf2-generated fixtures because font widths resolve correctly.
// ---------------------------------------------------------------------------

#[test]
fn accuracy_nics_firearm_checks() {
    let (cf1, wf1) = benchmark_pdf(
        "downloaded",
        "nics-firearm-checks.pdf",
        "nics-firearm-checks",
    )
    .expect("nics-firearm-checks.pdf should parse");
    print_text_summary("nics-firearm-checks.pdf", &cf1, &wf1);
    // Baseline: chars F1~0.89, words F1~0.92 — close to PRD target
    // PRD target: chars/words F1 >= 0.95
    assert!(
        cf1.f1 >= 0.80,
        "nics-firearm-checks chars F1 {:.3} < 0.80",
        cf1.f1
    );
    assert!(
        wf1.f1 >= 0.80,
        "nics-firearm-checks words F1 {:.3} < 0.80",
        wf1.f1
    );

    // Table accuracy
    let table_results = benchmark_tables(
        "downloaded",
        "nics-firearm-checks.pdf",
        "nics-firearm-checks",
    );
    print_table_summary("nics-firearm-checks.pdf", &table_results);
}

#[test]
fn accuracy_scotus_transcript() {
    // This PDF currently fails to parse (content stream issue).
    // Test documents the failure and skips gracefully.
    let result = benchmark_pdf(
        "downloaded",
        "scotus-transcript-p1.pdf",
        "scotus-transcript-p1",
    );
    match result {
        Some((cf1, wf1)) => {
            print_text_summary("scotus-transcript-p1.pdf", &cf1, &wf1);
            // PRD target: chars/words F1 >= 0.95
            assert!(
                cf1.f1 >= 0.50,
                "scotus-transcript chars F1 {:.3} < 0.50",
                cf1.f1
            );
        }
        None => {
            println!("scotus-transcript-p1.pdf: parse not yet supported, skipping assertions");
        }
    }
}

#[test]
fn accuracy_pdffill_demo() {
    let (cf1, wf1) = benchmark_pdf("downloaded", "pdffill-demo.pdf", "pdffill-demo")
        .expect("pdffill-demo.pdf should parse");
    print_text_summary("pdffill-demo.pdf", &cf1, &wf1);
    // Baseline: chars F1=1.000, words F1=0.958 — meets PRD target
    // PRD target: chars/words F1 >= 0.95
    assert!(cf1.f1 >= 0.95, "pdffill-demo chars F1 {:.3} < 0.95", cf1.f1);
    assert!(wf1.f1 >= 0.90, "pdffill-demo words F1 {:.3} < 0.90", wf1.f1);
}

// ---------------------------------------------------------------------------
// Aggregate summary test
// ---------------------------------------------------------------------------

#[test]
fn accuracy_aggregate_summary() {
    println!("\n========================================");
    println!("  Aggregate Accuracy Benchmark Summary");
    println!("========================================\n");

    let all_fixtures: &[(&str, &str, &str)] = &[
        ("generated", "basic_text.pdf", "basic_text"),
        ("generated", "multicolumn.pdf", "multicolumn"),
        ("generated", "table_lattice.pdf", "table_lattice"),
        ("generated", "table_borderless.pdf", "table_borderless"),
        ("generated", "table_merged_cells.pdf", "table_merged_cells"),
        ("generated", "multi_font.pdf", "multi_font"),
        ("generated", "cjk_mixed.pdf", "cjk_mixed"),
        ("generated", "long_document.pdf", "long_document"),
        ("generated", "annotations_links.pdf", "annotations_links"),
        (
            "downloaded",
            "nics-firearm-checks.pdf",
            "nics-firearm-checks",
        ),
        (
            "downloaded",
            "scotus-transcript-p1.pdf",
            "scotus-transcript-p1",
        ),
        ("downloaded", "pdffill-demo.pdf", "pdffill-demo"),
    ];

    let mut total_char_tp = 0usize;
    let mut total_char_golden = 0usize;
    let mut total_char_actual = 0usize;
    let mut total_word_tp = 0usize;
    let mut total_word_golden = 0usize;
    let mut total_word_actual = 0usize;
    let mut parsed = 0usize;
    let mut skipped = 0usize;

    for &(subdir, filename, stem) in all_fixtures {
        match benchmark_pdf(subdir, filename, stem) {
            Some((cf1, wf1)) => {
                print_text_summary(filename, &cf1, &wf1);
                println!();
                total_char_tp += cf1.true_positives;
                total_char_golden += cf1.golden_count;
                total_char_actual += cf1.actual_count;
                total_word_tp += wf1.true_positives;
                total_word_golden += wf1.golden_count;
                total_word_actual += wf1.actual_count;
                parsed += 1;
            }
            None => {
                skipped += 1;
            }
        }
    }

    let agg_char_p = total_char_tp as f64 / total_char_actual.max(1) as f64;
    let agg_char_r = total_char_tp as f64 / total_char_golden.max(1) as f64;
    let agg_char_f1 = if agg_char_p + agg_char_r == 0.0 {
        0.0
    } else {
        2.0 * agg_char_p * agg_char_r / (agg_char_p + agg_char_r)
    };

    let agg_word_p = total_word_tp as f64 / total_word_actual.max(1) as f64;
    let agg_word_r = total_word_tp as f64 / total_word_golden.max(1) as f64;
    let agg_word_f1 = if agg_word_p + agg_word_r == 0.0 {
        0.0
    } else {
        2.0 * agg_word_p * agg_word_r / (agg_word_p + agg_word_r)
    };

    println!("--- AGGREGATE ({parsed} parsed, {skipped} skipped) ---");
    println!(
        "Chars:  P={:.3}  R={:.3}  F1={:.3}  ({}/{} golden, {} actual)",
        agg_char_p, agg_char_r, agg_char_f1, total_char_tp, total_char_golden, total_char_actual
    );
    println!(
        "Words:  P={:.3}  R={:.3}  F1={:.3}  ({}/{} golden, {} actual)",
        agg_word_p, agg_word_r, agg_word_f1, total_word_tp, total_word_golden, total_word_actual
    );
    println!("PRD targets: chars/words F1 >= 0.95, lattice table >= 0.90");
}
